---
title: "Homework 2"
author: "Jason Freeberg and Nikolay Anguelov"
date: "23 October 2016"
output: pdf_document
---

Question 1
Part A
```{r}
set.seed(1)

library(class)
library(MASS)
library(data.table)
library(ggplot2)
library(tree)
library(ggdendro)

neatPrint <- function(listOfStrings){
  cat(listOfStrings, fill = 1)
}

data(iris)
iris = as.data.frame(iris)
X.iris = iris[, c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')]
Y.iris = iris[,'Species']

k2 <- knn(train= iris[,!(names(iris) %in% c("Species"))],
          test = iris[, !(names(iris) %in% c("Species"))],
          cl = iris$Species, k=2)

testresults <- data.table(iris$Species, k2)
denom = nrow(testresults)
k2correct <- sum(testresults$V1 == testresults$k2)
k2correct <- k2correct/denom
a <- paste("Training error rate with K=2...", round(1-k2correct, 4))
neatPrint(c(a))
```

Part B
```{r}
k2cv <- knn.cv(train = X.iris, cl=Y.iris, k=2)
pred0 <- data.frame(k2cv, iris$Species)
numincorrect0 <- sum(pred0$k2cv != pred0$iris.Species)
errorrate0 <- numincorrect0/nrow(pred0)
errorrate0
# COME BACK TO PART B... FIND OPTIMAL K
```

Part C
```{r}
fit <- lda(Species ~ ., data=iris, CV=TRUE)
pred1 <- data.frame(fit$class, iris$Species)
numincorrect <- sum(pred1$fit.class!=pred1$iris.Species)
errorrate1 <- numincorrect/nrow(pred1)
errorrate1
```

Part D
```{r}
fit2 <- qda(Species ~ ., data=iris, CV=TRUE)
pred2<- data.frame(fit2$class, iris$Species)
numincorrect2 <- sum(pred2$fit2.class != pred2$iris.Species)
errorrrate2 <- numincorrect2/nrow(pred2)
errorrrate2
```

Part E

LDA appears to be the strongest model because the test error rate is the lowest. It should be noted that QDA's test error rate was very clsoe to LDA's, so it is not 100% clear which model one should select.

Part F
```{r}
# Bootstrap 100 times
runs <- 1:100

# Lists to save the results
knnErrors = vector()
ldaErrors = vector()
qdaErrors = vector()

for (i in runs){
  print(i)
  sampleIndex <- sample(1:nrow(iris), size = nrow(iris), replace = TRUE)
  sampleData <- iris[sampleIndex, ]
  
  # Fit the three models
  # Use a different value for K?
  KNNfit <- knn.cv(train = sampleData[, 1:4], cl = sampleData[, 5], k = 2)
  LDAfit <- lda(Species ~ ., data = sampleData, CV = T)
  QDAfit <- qda(Species ~ ., data = sampleData, CV = T)
  
  # Compute error rates
  knnError <- sum(KNNfit != sampleData$Species) / nrow(sampleData)
  ldaError <- sum(LDAfit$class != sampleData$Species) / nrow(sampleData)
  qdaError <- sum(QDAfit$class != sampleData$Species) / nrow(sampleData)
  
  # Save error rates
  knnErrors[i] = knnError
  ldaErrors[i] = ldaError
  qdaErrors[i] = qdaError
}

# Aggregate results
knnMean <- mean(knnErrors)
knnVar <- var(knnErrors)

ldaMean <- mean(ldaErrors)
ldaVar <- var(ldaErrors)

qdaMean <- mean(qdaErrors)
qdaVar <- var(qdaErrors)

a <- paste("Mean KNN Error =", knnMean)
b <- paste("Variance of KNN Error =", knnVar)

c <- paste("Mean LDA Error =", ldaMean)
d <- paste("Variance of LDA Error =", ldaVar)

e <- paste("Mean QDA Error =", qdaMean)
f <- paste("Variance of QDA Error =", qdaVar)

neatPrint(c(a,b,c,d,e,f))
```

QDA achieved the lowest average error rate and lowest variance over the 100 bootstrap resamplings. Therefore we would choose to use QDA.

Part G
```{r}
dt = data.frame(Sepal.Length=c(4,6), Sepal.Width=c(2.5,4), Petal.Length=c(3,1.8), Petal.Width=c(0.5,1.5))

ldaModel <- lda(Species ~ ., data = iris)
qdaModel <- qda(Species ~ ., data = iris)

ldaLabels <- predict(ldaModel, dt)
qdaLabels <- predict(qdaModel, dt)
knnLabels <- knn(train = iris[, 1:4], cl = iris$Species, test = dt, k = 2) # Come back and edit best K
#### dsfIXDVHIUSIJOFVHUBGYDFHSAOJSVBYSAHUJOVFGUYSEDHWISEFT67W8Y2HIU3RB2EFY78GEYU2R3WFD

a <- paste("Predicted KNN labels =", paste(knnLabels, collapse = ', '))
b <- paste("Predicted LDA labels =", paste(ldaLabels$class, collapse = ", "))
c <- paste("Predicted QDA labels =", paste(qdaLabels$class, collapse = ", "))

neatPrint(c(a,b,c))
```


Question 2
Part A
```{r}
# Load data
spam <- read.table("spambase.dat", header = T, sep = "")
spam$y = factor(spam$y, levels=c(0,1), labels=c("good","spam"))

# Sample data indices, the seed is set in the first code chunk.
testIndices <- sample(1:nrow(spam), size = 1000)
trainIndices <- -testIndices

# Use index samples to select observations
spamtest <- spam[testIndices, ]
spamtrain <- spam[trainIndices, ]
```

Part B, 1
```{r}

basictree <- tree(spamtrain, formula = y ~ .)
# How do we prune the tree?? Do we use "prunedtree <- prune.tree(basictree)"? I didn't see a difference 
# For now, I will use the base tree as a placeholder.
prunedtree <- basictree
tree_data <- dendro_data(prunedtree)

# Clean ggplot of the data
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
                 colour = "blue") +
   geom_text(data = label(tree_data), 
              aes(x = x, y = y, label = label), vjust = -0.5, size = 3) +
    geom_text(data = leaf_label(tree_data), 
              aes(x = x, y = y, label = label), vjust = 0.5, size = 2) +
    theme_dendro()

# Merge the true values from spamtest and predicted values from model
pred3 <- as.data.frame(predict(prunedtree, newdata = spamtest, type = "class"))
pred3 <- data.frame(list("true" = spamtest$y, "predicted" = pred3[,1]))

# Sum all instances where the two are unequal, divide by total observations
treeError1 <- sum(pred3$true != pred3$predicted) / nrow(pred3)
```

Part B, 2
```{r}
# A tree with the control parameters
controltree <- tree(data = spamtrain, 
                   formula = y ~ ., 
                   control = tree.control(nrow(spamtrain), mincut = 2, minsize = 5, mindev = 0.001))

# Merge the true values from spamtest and predicted values from model
pred4 <- as.data.frame(predict(prunedtree, newdata = spamtest, type = "class"))
pred4 <- data.frame(list("true" = spamtest$y, "predicted" = pred4[,1]))

# Sum all instances where the two are unequal, divide by total observations
treeError2 <- sum(pred4$true != pred4$predicted) / nrow(pred4)
```

Part B, 3
```{r}
# Random forest

```

Part B, 5
```{r}
# Skip the 4th bullet point because that's for 231 students.

# Make the model
logit = glm(data = spamtrain, formula = y ~ ., family = binomial(link = "logit"))

# Merge the true values from spamtest and predicted values from model
pred6 <- predict(object = logit, newdata = spamtest)
pred6 <- data.frame(list("true" = spamtest$y, "predicted" = pred4[,1]))

logitError <- sum(pred6$true != pred6$predicted) / nrow(pred6)
```


Question 3
Part A
```{r}
seeds =
  read.table('http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt')
names(seeds) = c('area', 'perimeter', 'compactness', 'length',
                 'width', 'asymmetry', 'groovelength', 'type')
seeds.label = factor(seeds$type)
seeds.orig = seeds[,-ncol(seeds)]

seeds = as.data.frame(scale(seeds.orig))
seeds.pca <- prcomp(seeds)
summary(seeds.pca)
cor(seeds)
```

The proportion of vairance explained by PC1 and PC2 = 0.8898.

Part B
```{r}
pcaLoadings <- as.data.frame(seeds.pca$x)
kmeans3 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])
kmeans4 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])
kmeans5 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])

seeds$k3 <- as.factor(kmeans3$cluster)
seeds$k4 <- kmeans4$cluster
seeds$k5 <- kmeans5$cluster

plot1 <- ggplot(data = pcaLoadings) +
  geom_point(mapping = aes(x= PC1, y = PC2, color = seeds$k3)) 
  # ADD CLUSTER CENTERS

# More plots
```

Part C
```{r}
# 
PCAdist <- dist(x = pcaLoadings[,c(1,2)])

complete <- hclust(d = PCAdist, method = "complete")
single <- hclust(d = PCAdist, method = "single")
average <- hclust(d = PCAdist, method = "average")

ggdendrogram(complete)
ggdendrogram(single)
ggdendrogram(average)

# Cut complete linkage object
cluster3 <- cutree(complete, k = 3)
cluster4 <- cutree(complete, k = 4)
cluster5 <- cutree(complete, k = 5)

plotc3 <- ggplot(data = pcaLoadings) +
  geom_point(mapping = aes(x= PC1, y = PC2, color = as.factor(cluster3))) +
  theme

plotc4 <- ggplot(data = pcaLoadings) +
  geom_point(mapping = aes(x= PC1, y = PC2, color = as.factor(cluster4))) 

plotc5 <- ggplot(data = pcaLoadings) +
  geom_point(mapping = aes(x= PC1, y = PC2, color = as.factor(cluster5))) 
```

