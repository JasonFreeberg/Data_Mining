---
title: "Homework 2"
author: "Jason Freeberg and Nikolay Anguelov"
date: "23 October 2016"
output: pdf_document
---

Question 1
Part A
```{r}
set.seed(1)

library(class)
library(MASS)
library(data.table)
library(ggplot2)
library(tree)
library(ggdendro)

neatPrint <- function(listOfStrings){
  cat(listOfStrings, fill = 1)
}

data(iris)
iris = as.data.frame(iris)
X.iris = iris[, c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')]
Y.iris = iris[,'Species']

k2 <- knn(train= iris[,!(names(iris) %in% c("Species"))],
          test = iris[, !(names(iris) %in% c("Species"))],
          cl = iris$Species, k=2)

testresults <- data.table(iris$Species, k2)
denom = nrow(testresults)
k2correct <- sum(testresults$V1 == testresults$k2)
k2correct <- k2correct/denom
a <- paste("Training error rate with K=2...", round(1-k2correct, 4))
neatPrint(c(a))
```

Part B
```{r}
k2cv <- knn.cv(train = X.iris, cl=Y.iris, k=2)
pred0 <- data.frame(k2cv, iris$Species)
numincorrect0 <- sum(pred0$k2cv != pred0$iris.Species)
errorrate0 <- numincorrect0/nrow(pred0)
errorrate0
#COME BACK TO PART B... FIND OPTIMAL K
```

Part C
```{r}
fit <- lda(Species ~ ., data=iris, CV=TRUE)
pred1 <- data.frame(fit$class, iris$Species)
numincorrect <- sum(pred$fit.class!=pred$iris.Species)
errorrate <- numincorrect/nrow(pred1)
```

Part D
```{r}
fit2 <- qda(Species ~ ., data=iris, CV=TRUE)
pred2<- data.frame(fit2$class, iris$Species)
numincorrect2 <- sum(pred2$fit2.class != pred2$iris.Species)
errorrrate2 <- numincorrect2/nrow(pred2)
errorrrate2
```

Part E

LDA appears to be the strongest model because the test error rate is the lowest. It should be noted that QDA's test error rate was very clsoe to LDA's, so it is not 100% clear which model one should select.

Part F
```{r}


```

Question 2
Part A
```{r}
# Load data
spam <- read.table("spambase.dat", header = T, sep = "")
spam$y = factor(spam$y, levels=c(0,1), labels=c("good","spam"))

# Sample data indices, the seed is set in the first code chunk.
testIndices <- sample(1:nrow(spam), size = 1000)
trainIndices <- -testIndices

# Use index samples to select observations
spamtest <- spam[testIndices, ]
spamtrain <- spam[trainIndices, ]
```

Part B, 1
```{r}

basictree <- tree(spamtrain, formula = y ~ .)
# How do we prune the tree?? Do we use "prunedtree <- prune.tree(basictree)"? I didn't see a difference 
# For now, I will use the base tree as a placeholder.
prunedtree <- basictree
tree_data <- dendro_data(prunedtree)

# Clean ggplot of the data
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
                 colour = "blue") +
   geom_text(data = label(tree_data), 
              aes(x = x, y = y, label = label), vjust = -0.5, size = 3) +
    geom_text(data = leaf_label(tree_data), 
              aes(x = x, y = y, label = label), vjust = 0.5, size = 2) +
    theme_dendro()

# Merge the true values from spamtest and predicted values from model
pred3 <- as.data.frame(predict(prunedtree, newdata = spamtest, type = "class"))
pred3 <- data.frame(list("true" = spamtest$y, "predicted" = pred3[,1]))

# Sum all instances where the two are unequal, divide by total observations
treeError1 <- sum(pred3$true != pred3$predicted) / nrow(pred3)
```

Part B, 2
```{r}
# A tree with the control parameters
controltree <- tree(data = spamtrain, 
                   formula = y ~ ., 
                   control = tree.control(nrow(spamtrain), mincut = 2, minsize = 5, mindev = 0.001))

# Merge the true values from spamtest and predicted values from model
pred4 <- as.data.frame(predict(prunedtree, newdata = spamtest, type = "class"))
pred4 <- data.frame(list("true" = spamtest$y, "predicted" = pred4[,1]))

# Sum all instances where the two are unequal, divide by total observations
treeError2 <- sum(pred4$true != pred4$predicted) / nrow(pred4)
```

Part B, 3
```{r}
# Random forest

```

Part B, 5
```{r}
# Skip the 4th bullet point because that's for 231 students.

# Make the model
logit = glm(data = spamtrain, formula = y ~ ., family = binomial(link = "logit"))

# Merge the true values from spamtest and predicted values from model
pred6 <- predict(object = logit, newdata = spamtest)
pred6 <- data.frame(list("true" = spamtest$y, "predicted" = pred4[,1]))

logitError <- sum(pred6$true != pred6$predicted) / nrow(pred6)
```


Question 3
Part A
```{r}
seeds =
  read.table('http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt')
names(seeds) = c('area', 'perimeter', 'compactness', 'length',
                 'width', 'asymmetry', 'groovelength', 'type')
seeds.label = factor(seeds$type)
seeds.orig = seeds[,-ncol(seeds)]

seeds = as.data.frame(scale(seeds.orig))
seeds.pca <- prcomp(seeds)
summary(seeds.pca)
cor(seeds)
```

The proportion of vairance explained by PC1 and PC2 = 0.98668.

Part B
```{r}
pcaLoadings <- as.data.frame(seeds.pca$x)
kmeans3 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])
kmeans4 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])
kmeans5 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])

seeds$k3 <- as.factor(kmeans3$cluster)
seeds$k4 <- kmeans4$cluster
seeds$k5 <- kmeans5$cluster

plot1 <- ggplot(data = pcaLoadings) +
  geom_point(mapping = aes(x= PC1, y = PC2, color = seeds$k3)) 
  # ADD CLUSTER CENTERS

# More plots
```

Part C
```{r}


```

