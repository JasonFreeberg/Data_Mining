---
title: "Homework 2"
author: "Jason Freeberg and Nikolay Anguelov"
date: "23 October 2016"
output: pdf_document
---

Question 1
Part A
```{r}
library(class)
library(MASS)
library(data.table)
library(ggplot2)

neatPrint <- function(listOfStrings){
  cat(listOfStrings, fill = 1)
}

data(iris)
iris = as.data.frame(iris)
X.iris = iris[, c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')]
Y.iris = iris[,'Species']

k2 <- knn(train= iris[,!(names(iris) %in% c("Species"))],
          test = iris[, !(names(iris) %in% c("Species"))],
          cl = iris$Species, k=2)

testresults <- data.table(iris$Species, k2)
denom = nrow(testresults)
k2correct <- sum(testresults$V1 == testresults$k2)
k2correct <- k2correct/denom
a <- paste("Training error rate with K=2...", round(1-k2correct, 4))
neatPrint(c(a))
```

Part B
```{r}
k2cv <- knn.cv(train = X.iris, cl=Y.iris, k=2)
pred0 <- data.frame(k2cv, iris$Species)
numincorrect0 <- sum(pred0$k2cv != pred0$iris.Species)
errorrate0 <- numincorrect0/nrow(pred0)
errorrate0
#COME BACK TO PART B... FIND OPTIMAL K
```

Part C
```{r}
fit <- lda(Species ~ ., data=iris, CV=TRUE)
pred <- data.frame(fit$class, iris$Species)
numincorrect <- sum(pred$fit.class!=pred$iris.Species)
errorrate <- numincorrect/nrow(pred)
```

Part D
```{r}
fit2 <- qda(Species ~ ., data=iris, CV=TRUE)
pred2<- data.frame(fit2$class, iris$Species)
numincorrect2 <- sum(pred2$fit2.class != pred2$iris.Species)
errorrrate2 <- numincorrect2/nrow(pred2)
errorrrate2
```

Part E

LDA appears to be the strongest model because the test error rate is the lowest. It should be noted that QDA's test error rate was very clsoe to LDA's, so it is not 100% clear which model one should select.

Part F
```{r}


```

Question 2
Part A
...
...
...


Question 3
Part A
```{r}
seeds =
  read.table('http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt')
names(seeds) = c('area', 'perimeter', 'compactness', 'length',
                 'width', 'asymmetry', 'groovelength', 'type')
seeds.label = factor(seeds$type)
seeds.orig = seeds[,-ncol(seeds)]

seeds = as.data.frame(scale(seeds.orig))
seeds.pca <- prcomp(seeds)
summary(seeds.pca)
cor(seeds)
```

The proportion of vairance explained by PC1 and PC2 = 0.98668.

Part B
```{r}
pcaLoadings <- as.data.frame(seeds.pca$x)
kmeans3 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])
kmeans4 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])
kmeans5 <- kmeans(centers = 3, x = pcaLoadings[, c(1,2)])

seeds$k3 <- as.factor(kmeans3$cluster)
seeds$k4 <- kmeans4$cluster
seeds$k5 <- kmeans5$cluster

plot1 <- ggplot(data = pcaLoadings) +
  geom_point(mapping = aes(x= PC1, y = PC2, color = seeds$k3)) 
  # ADD CLUSTER CENTERS

# More plots
```

Part C
```{r}


```

